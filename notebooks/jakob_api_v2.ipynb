{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install datetime --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54e395",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6011c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d702e6d",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56762f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(filename):\n",
    "    data = pd.read_csv(filename).drop(columns = 'Unnamed: 0')\n",
    "    data['Content'] = None\n",
    "    return data\n",
    "\n",
    "def get_url(cellar_ref, doctype=\"03\"):\n",
    "    psid = cellar_ref\n",
    "    psname = \"cellar\" # other options: cellar, celex, oj...\n",
    "    lancode = \"0006\" # language code\n",
    "    doctype = doctype # default: 03\n",
    "    docnum = \"DOC_1\"\n",
    "    # for further information, see Documentation Page 37: https://op.europa.eu/en/publication-detail/-/publication/50ecce27-857e-11e8-ac6a-01aa75ed71a1/language-en/format-PDF/source-73059305\n",
    "    return f\"http://publications.europa.eu/resource/{psname}/{psid}.{lancode}.{doctype}/{docnum}\"\n",
    "\n",
    "def get_content(URL):\n",
    "    response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "    # one minor bug still in there: some requests (for example number 58 in 20220601_larger_data_b) are a valid request but have to download many mb first. the solution would be to stop the request.get if it runs longer than x seconds\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        if str(soup)[1:4] == \"PDF\":\n",
    "            '''\n",
    "            in some (few) cases, the doctype is not 03 but 02. change it for these cases\n",
    "            '''\n",
    "            print(\"pdf detected, but fixed\")\n",
    "            doctype = '02'\n",
    "            URL = URL[:-8] + doctype + URL[-6:]\n",
    "            response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        else:\n",
    "            print(\"no problem here\")\n",
    "            doctype = '03'\n",
    "    except:\n",
    "        '''\n",
    "        in case there is an error\n",
    "        '''\n",
    "        print(\"yes problem here\")\n",
    "        URL = URL[:-8] + '02' + URL[-6:]\n",
    "        response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "    if soup.find(\"p\", class_=\"oj-normal\") == None:\n",
    "        content = ' '.join([item.text for item in soup.find_all(\"p\", class_=\"normal\")])\n",
    "    else:\n",
    "        content = ' '.join([item.text for item in soup.find_all(\"p\", class_=\"oj-normal\")])\n",
    "    return content #.split('Whereas:', 1)[1] # only return text without the head\n",
    "\n",
    "def clean_data(data):\n",
    "    data = data[data['Content'] != \"\"]\n",
    "    data = data[data['Content'].str[0:3] == 'THE'] #remove content in other languages\n",
    "    data = data[data['Content'].str.contains('Whereas: ')] # contains the split word\n",
    "    data.loc[:, 'Content'] = data['Content'].apply(lambda x: x.split('Whereas: ', 1)[1]) # split off header\n",
    "    data = data[data['Content'].str[0:3] == \"(1)\"] #gotta make sure it's standardized!\n",
    "    return data.reset_index().drop(columns = \"index\")\n",
    "\n",
    "def get_all_content(data):\n",
    "    cellar_references = data['cellar']    \n",
    "    for index, ref in enumerate(cellar_references):\n",
    "        data.loc[index, 'Content'] = get_content(get_url(ref))\n",
    "        print(f'Row {index} with cellar-number {ref} done')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a5c0a",
   "metadata": {},
   "source": [
    "**Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ce720",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#retrieve metadata\n",
    "filename = \"../raw_data/20220601_larger_data_b.csv\"\n",
    "metadata = read_metadata(filename)\n",
    "\n",
    "# subset metadata\n",
    "metadata_subset = metadata.iloc[0:10] # somehow the formatting is off when I subset it..\n",
    "\n",
    "# get content\n",
    "data_with_content = get_all_content(metadata_subset)\n",
    "\n",
    "# clean content\n",
    "data_with_content_clean = clean_data(data_with_content)\n",
    "\n",
    "# export data to csv\n",
    "#data_b_with_content.to_csv(\"../raw_data/20220601_larger_data_b_scraped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56320d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83305ecc",
   "metadata": {},
   "source": [
    "**Read already processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46977907",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_data/20220601_larger_data_b_scraped_clean.csv\")\n",
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../raw_data/20220601_larger_data_b_scraped_clean_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd1425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21387bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url(data['cellar'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474cc478",
   "metadata": {},
   "source": [
    "**Test Area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.iloc[3498:3502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a423e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_metadata(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9350ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = get_url(metadata['cellar'][58], \"03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "try:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    if str(soup)[1:4] == \"PDF\":\n",
    "        print(\"pdf detected, but fixed\")\n",
    "        '''\n",
    "        in some (few) cases, the doctype is not 03 but 02. change it for these cases\n",
    "        '''\n",
    "        URL = URL[:-8] + '02' + URL[-6:]\n",
    "        response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    else:\n",
    "        print(\"no problem here\")\n",
    "except:\n",
    "    '''\n",
    "    in case there is an error\n",
    "    '''\n",
    "    print(\"yes problem here\")\n",
    "    URL = URL[:-8] + '02' + URL[-6:]\n",
    "    response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "if soup.find(\"p\", class_=\"oj-normal\") == None:\n",
    "    content = ' '.join([item.text for item in soup.find_all(\"p\", class_=\"normal\")])\n",
    "else:\n",
    "    content = ' '.join([item.text for item in soup.find_all(\"p\", class_=\"oj-normal\")])\n",
    "\n",
    "print(URL)\n",
    "\n",
    "content#.split('Whereas:', 1)[1] # only return text without the head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

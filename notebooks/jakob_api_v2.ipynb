{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install datetime --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54e395",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6011c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d702e6d",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56762f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(filename):\n",
    "    '''\n",
    "    reads in metadata from the api.py file and adds an empty column where the content of the pages will be in the end\n",
    "    '''\n",
    "    data = pd.read_csv(filename)\n",
    "    data['Content'] = None\n",
    "    return data\n",
    "\n",
    "def get_url(cellar_ref, doctype=\"03\"):\n",
    "    '''\n",
    "    creates a url based on the cellar reference in the metadata, which will be used to scrape the content\n",
    "    '''\n",
    "    psid = cellar_ref\n",
    "    psname = \"cellar\" # other options: cellar, celex, oj...\n",
    "    lancode = \"0006\" # language code\n",
    "    doctype = doctype # default: 03\n",
    "    docnum = \"DOC_1\"\n",
    "    # for further information, see Documentation Page 37: https://op.europa.eu/en/publication-detail/-/publication/50ecce27-857e-11e8-ac6a-01aa75ed71a1/language-en/format-PDF/source-73059305\n",
    "    return f\"http://publications.europa.eu/resource/{psname}/{psid}.{lancode}.{doctype}/{docnum}\"\n",
    "\n",
    "def get_content(URL):\n",
    "    '''\n",
    "    main function, scrapes content. added some code to catch errors.\n",
    "    '''\n",
    "    response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "    # one minor bug still in there: some requests (for example number 58 in 20220601_larger_data_b) are a valid request but have to download many mb first. the solution would be to stop the request.get if it runs longer than x seconds\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        if str(soup)[1:4] == \"PDF\":\n",
    "            '''\n",
    "            in some (few) cases, the doctype is not 03 but 02. change it for these cases\n",
    "            '''\n",
    "            print(\"pdf detected, but fixed\")\n",
    "            doctype = '02'\n",
    "            URL = URL[:-8] + doctype + URL[-6:]\n",
    "            response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        else:\n",
    "            print(\"no problem here\")\n",
    "            doctype = '03'\n",
    "    except:\n",
    "        '''\n",
    "        in case there is an error\n",
    "        '''\n",
    "        print(\"yes problem here\")\n",
    "        URL = URL[:-8] + '02' + URL[-6:]\n",
    "        response = requests.get(URL, headers={\"Accept-Language\":\"en-US\"})\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "    if soup.find(\"p\", class_=\"oj-normal\") == None:\n",
    "        content = ' '.join([item.text for item in soup.find_all(\"p\", class_=\"normal\")])\n",
    "    else:\n",
    "        content = ' '.join([item.text for item in soup.find_all(\"p\", class_=\"oj-normal\")])\n",
    "    return content\n",
    "\n",
    "def clean_data(data):\n",
    "    '''\n",
    "    takes scraped data and removes rows which contain, no information, information in non-english and the head of all the valid content\n",
    "    '''\n",
    "    data = data[data['Content'] != \"\"]\n",
    "    data = data[data['Content'].str[0:3] == 'THE'] #remove content in other languages\n",
    "    data = data[data['Content'].str.contains('Whereas: ')] # contains the split word\n",
    "    data.loc[:, 'Content'] = data['Content'].apply(lambda x: x.split('Whereas: ', 1)[1]) # split off header\n",
    "    data = data[data['Content'].str[0:3] == \"(1)\"] #gotta make sure it's standardized!\n",
    "    return data.reset_index().drop(columns = \"index\")\n",
    "\n",
    "def get_all_content(data):\n",
    "    '''\n",
    "    loops over the functions to get all content\n",
    "    '''\n",
    "    cellar_references = data['cellar']    \n",
    "    for index, ref in enumerate(cellar_references):\n",
    "        data.loc[index, 'Content'] = get_content(get_url(ref))\n",
    "        print(f'Row {index} with cellar-number {ref} done')\n",
    "    return data\n",
    "\n",
    "def get_all_content_with_splitting(data, batchsize=500, path =\"../raw_data/\", filename_without_csv=\"20220602\"):\n",
    "    '''\n",
    "    loops over the functions to get all content. for more than 500 files the process will be split up\n",
    "    '''\n",
    "    if len(data) > batchsize:\n",
    "        remaining = len(data) % batchsize\n",
    "        iterations = int((len(data)-remaining) / batchsize)\n",
    "        for batch in range(0,iterations):\n",
    "            tmp = data.iloc[batch*batchsize:(batch+1)*batchsize]\n",
    "            cellar_references = tmp['cellar'] \n",
    "            for index, ref in enumerate(cellar_references):\n",
    "                tmp.loc[index, 'Content'] = get_content(get_url(ref))\n",
    "                print(f'Row {index} from batch {batch} with cellar-number {ref} done')\n",
    "                tmp.to_csv(f\"{path}{filename}_tmp_batch{batch}.csv\")\n",
    "        # add remaining rows\n",
    "        #return \"partitioned data can be found in '../raw_data/'\n",
    "    else:\n",
    "        cellar_references = data['cellar']    \n",
    "        for index, ref in enumerate(cellar_references):\n",
    "            data.loc[index, 'Content'] = get_content(get_url(ref))\n",
    "            print(f'Row {index} with cellar-number {ref} done')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a5c0a",
   "metadata": {},
   "source": [
    "**Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ce720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#retrieve metadata\n",
    "path = \"../raw_data/\"\n",
    "filename_without_csv = \"20220602\"\n",
    "data = read_metadata(path + filename_without_csv + '.csv')\n",
    "\n",
    "# subset metadata\n",
    "data = data.iloc[:50]\n",
    "\n",
    "# get content\n",
    "data_with_content = get_all_content_with_splitting(data, batchsize = 10)\n",
    "\n",
    "# clean content\n",
    "#data_with_content_clean = clean_data(data_with_content)\n",
    "\n",
    "# export data to csv\n",
    "#data_with_content_clean.to_csv(path + filename_without_csv + \"_scraped_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc27c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83305ecc",
   "metadata": {},
   "source": [
    "**Read already processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46977907",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_data/20220602_scraped_test.csv\")\n",
    "#data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff13089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../raw_data/20220602_scraped_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd1425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474cc478",
   "metadata": {},
   "source": [
    "**Test Area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21387bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url(data['cellar'][100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

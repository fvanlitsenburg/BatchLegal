{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install plotly --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_metadata_for_vis(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data[~data[\"dir_1\"].isna()].reset_index().drop(columns = \"index\") # drop rows that have NA in dir_1 column\n",
    "    return data\n",
    "    \n",
    "def exploration_histogram(data):\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    return sns.histplot(data['date'], bins = 50)\n",
    "\n",
    "def exploration_valcounts(data, directory=1):\n",
    "    '''\n",
    "    directories with the number of publications\n",
    "    '''\n",
    "    return data[f'dir_{str(directory)}'].value_counts()\n",
    "\n",
    "def exploration_show_subdirs(data, directory_level=1):\n",
    "    '''\n",
    "    print possible subdir-keywords\n",
    "    '''\n",
    "    #data = load_metadata_for_vis(filename)\n",
    "    for i in range(1, directory_level+1):\n",
    "        print(f\"Directory Level {str(i)}: {data[f'dir_{str(i)}'].value_counts().index}\")\n",
    "    return \"No return in this function\"\n",
    "\n",
    "def subset_data_subdirs(data, dir_1=None, dir_2=None, dir_3=None):\n",
    "    '''\n",
    "    used for visualizing the sub-directories - run \"exploration_show_subdirs for the possible keywords\"\n",
    "    '''\n",
    "    # create directory-index for the selected layer\n",
    "    if dir_1 == None:\n",
    "        print(\"no subsetting done, select keyword for dir_1 or 2\")\n",
    "        return data, 1\n",
    "    elif np.logical_and(dir_1 != None, dir_2 == None):\n",
    "        dir_keyword = dir_1\n",
    "        dir_string = 'dir_1'\n",
    "    elif np.logical_and(np.logical_and(dir_1 != None, dir_2 != None), dir_3 == None):\n",
    "        dir_keyword = dir_2\n",
    "        dir_string = 'dir_2'\n",
    "    elif dir_3 != None:\n",
    "        dir_keyword = dir_3\n",
    "        dir_string = 'dir_3'\n",
    "    dir_index = data[dir_string].value_counts().index\n",
    "    return data[data[dir_string] == dir_keyword], int(dir_string[-1])+1\n",
    "\n",
    "def subset_data(data, start_date=\"2011-01-01\", end_date=\"2021-12-31\", timesampling=\"Y\", directory_level=1):\n",
    "    '''\n",
    "    filtering dir_1 data on start- and end-time and whether 1 datapoint per month or per year\n",
    "    '''\n",
    "    start_date = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    data_subset = data[np.logical_and(data['date'] >= start_date, data['date'] <= end_date)]\n",
    "    directory_code = f\"dir_{directory_level}\"\n",
    "    # categories (dir_1) in descending frequencies\n",
    "    dir_index = data[directory_code].value_counts().index\n",
    "    #create dataframe for the others to append to and rename col to dir name\n",
    "    try:   \n",
    "        df = data_subset[data_subset[directory_code] == dir_index[0]].resample(timesampling, on='date')['title'].count().reset_index().rename(columns={'title':dir_index[0]})\n",
    "    except IndexError:\n",
    "        print(\"No Sub-Directories for this Keyword\")\n",
    "        return None\n",
    "    # create dataframe with publications per directory\n",
    "    for i in range(1,len(dir_index)):\n",
    "        category = dir_index[i]\n",
    "        temp = data_subset[data_subset[directory_code] == category].resample(timesampling, on='date')['title'].count().reset_index().rename(columns={'title':category})\n",
    "        df = df.merge(temp, how='left', on='date').fillna(0)\n",
    "    data_publications = pd.concat([df['date'], df.drop(columns = \"date\").astype('Int64')], axis=1)\n",
    "    return data_publications\n",
    "\n",
    "def visualization_piechart(data_publications):\n",
    "    '''\n",
    "    Comparison of Directory Frequency in Pie Chart\n",
    "    '''\n",
    "    piedata = data_publications.drop(columns='date').sum().reset_index()\n",
    "    fig = px.pie(piedata, values=0, names='index', title='Directories of published documents')\n",
    "    return fig.show()\n",
    "\n",
    "def visualization_stackedarea(data_publications, plottype=\"plotly\"):\n",
    "    '''\n",
    "    stacked area plot in either plotly (interactive) or matplotlib\n",
    "    '''\n",
    "    # prepare data\n",
    "    x = data_publications['date'].tolist() \n",
    "    y = data_publications.drop(columns = {\"date\"}).T.values.tolist()\n",
    "    labels = data_publications.drop(columns = {\"date\"})\n",
    "    # matplotlib\n",
    "    if plottype == \"matplotlib\":\n",
    "        fig = plt.figure(figsize=(12,7))\n",
    "        plt.stackplot(x,y, labels=labels)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Date of Publication\")\n",
    "        plt.ylabel(\"Number of Publications\")\n",
    "        plt.title(f\"Publication of EU-Regulations per Directory (stacked)\")\n",
    "        return plt.show()     \n",
    "    # plotly\n",
    "    elif plottype == \"plotly\":\n",
    "        # create dict for the labels in plotly\n",
    "        newnames = {}\n",
    "        for index in range(0,len(labels.columns)):\n",
    "            newnames[f\"wide_variable_{str(index)}\"] = labels.columns[index]\n",
    "        # plot\n",
    "        x_plot = x.copy()\n",
    "        y_plot = y.copy()\n",
    "        fig = px.area(x=x_plot, y=y_plot,\n",
    "                      labels={\"x\": \"Date of Publication\",\n",
    "                             \"value\": \"Number of Publications\",\n",
    "                             \"variable\": \"Category\"},\n",
    "                      title='Publication of EU-Regulations per Directory (stacked)')\n",
    "        fig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n",
    "                                              legendgroup = newnames[t.name],\n",
    "                                              hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])))\n",
    "        return fig.show()\n",
    "    else:\n",
    "        return \"please select either 'matplotlib' or 'plotly' as plottype\"\n",
    "    \n",
    "def visualization_stackedarea_normalized(data_publications, plottype=\"plotly\"):\n",
    "    '''\n",
    "    normalized stacked area plot in either plotly (interactive) or matplotlib\n",
    "    '''\n",
    "    #normalize \n",
    "    df = data_publications.drop(columns = {'date'})\n",
    "    data_publications_normalized = df.div(df.sum(axis=1), axis=0)\n",
    "    y_norm = data_publications_normalized.T.values.tolist()\n",
    "    # prepare data\n",
    "    x_norm = data_publications['date'].tolist() \n",
    "    labels = data_publications.drop(columns = {\"date\"})\n",
    "    # matplotlib\n",
    "    if plottype == \"matplotlib\":\n",
    "        # matplotlib\n",
    "        fig = plt.figure(figsize=(12,7))\n",
    "        plt.stackplot(x_norm, y_norm, labels=labels)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Date of Publication\")\n",
    "        plt.ylabel(\"Share of Publications in this Directory\")\n",
    "        plt.title(f\"Publication of EU-Regulations per Directory (stacked and normalized)\")\n",
    "        return plt.show()\n",
    "    # plotly\n",
    "    elif plottype == \"plotly\":\n",
    "        # create dict for the labels in plotly\n",
    "        newnames = {}\n",
    "        for index in range(0,len(labels.columns)):\n",
    "            newnames[f\"wide_variable_{str(index)}\"] = labels.columns[index]\n",
    "        # plot\n",
    "        fig = px.area(x=x_norm, y=y_norm,\n",
    "              labels={\"x\": \"Date of Publication\",\n",
    "                     \"value\": \"Share of Publications in this Directory\",\n",
    "                     \"variable\": \"Category\"},\n",
    "              title='Publication of EU-Regulations per Directory (stacked and normalized)')\n",
    "        fig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n",
    "                                              legendgroup = newnames[t.name],\n",
    "                                              hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])))\n",
    "        return fig.show()\n",
    "    else:\n",
    "        return \"please select either 'matplotlib' or 'plotly' as plottype\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdcb97",
   "metadata": {},
   "source": [
    "**Test Area** (Workflow below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../raw_data/20220602.csv\"\n",
    "data = load_metadata_for_vis(filename)\n",
    "data, dirlevel = subset_data_subdirs(data, dir_1='Agriculture', dir_2='Approximation of laws and health measures', dir_3='Plant health')\n",
    "#data, dirlevel = subset_data_subdirs(data, dir_1='Agriculture', dir_2='Statistics', dir_3=None)\n",
    "data = subset_data(data, start_date=\"2011-01-01\", end_date=\"2021-12-31\", timesampling=\"Y\", directory_level=dirlevel)\n",
    "visualization_piechart(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37532f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_data(data, start_date=\"2011-01-01\", end_date=\"2021-12-31\", timesampling=\"Y\", directory_level=1):\n",
    "    '''\n",
    "    filtering dir_1 data on start- and end-time and whether 1 datapoint per month or per year\n",
    "    '''\n",
    "    start_date = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    data_subset = data[np.logical_and(data['date'] >= start_date, data['date'] <= end_date)]\n",
    "    directory_code = f\"dir_{directory_level}\"\n",
    "    # categories (dir_1) in descending frequencies\n",
    "    dir_index = data[directory_code].value_counts().index\n",
    "    #create dataframe for the others to append to and rename col to dir name\n",
    "    df = data_subset[data_subset[directory_code] == dir_index[0]].resample(timesampling, on='date')['title'].count().reset_index().rename(columns={'title':dir_index[0]})\n",
    "    # create dataframe with publications per directory\n",
    "    for i in range(1,len(dir_index)):\n",
    "        category = dir_index[i]\n",
    "        temp = data_subset[data_subset[directory_code] == category].resample(timesampling, on='date')['title'].count().reset_index().rename(columns={'title':category})\n",
    "        df = df.merge(temp, how='left', on='date').fillna(0)\n",
    "    data_publications = pd.concat([df['date'], df.drop(columns = \"date\").astype('Int64')], axis=1)\n",
    "    return data_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3813a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "206041c6",
   "metadata": {},
   "source": [
    "**Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../raw_data/20220602.csv\"\n",
    "data = load_metadata_for_vis(filename)\n",
    "#exploration_histogram(data)\n",
    "#exploration_valcounts(data, directory=1)\n",
    "#exploration_show_subdirs(data, directory_level=1)\n",
    "data, dirlevel = subset_data_subdirs(data, dir_1='Fisheries', dir_2=None, dir_3=None)\n",
    "data = subset_data_new(data, start_date=\"2011-01-01\", end_date=\"2021-12-31\", timesampling=\"Y\", directory_level=dirlevel)\n",
    "visualization_piechart(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_stackedarea(data, plottype=\"plotly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a94316",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_stackedarea_normalized(data, plottype=\"plotly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

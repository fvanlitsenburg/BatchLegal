{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b9035f",
   "metadata": {},
   "source": [
    "# Batch Legal Mockup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e5bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0452bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading html-file + converting it into txt\n",
    "\n",
    "file = open(\"../../raw_data/test_data.html\", \"r\")\n",
    "data = BeautifulSoup(file)\n",
    "as_txt = data.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abf8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the sentences\n",
    "\n",
    "as_sentences = sent_tokenize(as_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d8f8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\nConsolidated TEXT: 32004L0038 — EN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Union's institutions do not assume any lia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The authentic versions of the relevant acts, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Those official texts are directly accessible t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article 2\\nDefinitions\\nFor the purposes of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  \\n\\n\\n\\n\\n\\nConsolidated TEXT: 32004L0038 — EN...\n",
       "1  The Union's institutions do not assume any lia...\n",
       "2  The authentic versions of the relevant acts, i...\n",
       "3  Those official texts are directly accessible t...\n",
       "4  Article 2\\nDefinitions\\nFor the purposes of th..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transforming sentences into DF\n",
    "\n",
    "txt_df = pd.DataFrame(as_sentences)\n",
    "txt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc3cfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80f77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Davy's Preproc-Function\n",
    "\n",
    "def cleaning(sentence):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercasing \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## removing numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## removing punctuation\n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenizing \n",
    "    stop_words = set(stopwords.words('english')) ## defining stopwords\n",
    "    tokenized_sentence_cleaned = [w for w in tokenized_sentence \n",
    "                                  if not w in stop_words] ## remove stopwords\n",
    "    lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"v\")  # v --> verbs\n",
    "              for word in tokenized_sentence_cleaned]\n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfe7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Davy's Function\n",
    "\n",
    "clean_txt = txt_df[0].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc7b1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      consolidate text l — en — l — en — — text mean...\n",
       "1           unions institutions assume liability content\n",
       "2      authentic versions relevant act include preamb...\n",
       "3      official texts directly accessible link embed ...\n",
       "4                  article definitions purpose directive\n",
       "                             ...                        \n",
       "234        methods make reference shall lay member state\n",
       "235                                                     \n",
       "236    member state shall communicate commission text...\n",
       "237    article entry force directive shall enter forc...\n",
       "238    article addressees directive address member state\n",
       "Name: 0, Length: 239, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking outcome of Preprocessing\n",
    "clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a7ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing data\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_text = vectorizer.fit_transform(clean_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e31fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling\n",
    "\n",
    "# Instantiating the LDA \n",
    "n_components = 3\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components, max_iter = 100)\n",
    "\n",
    "# Fitting the LDA on the vectorized documents\n",
    "lda_model.fit(vectorized_text)\n",
    "\n",
    "# Getting topics\n",
    "topics = lda_model.transform(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f702cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic model function from ML-10-lecture\n",
    "def print_topics(model, vectorizer, top_words):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"-\"*20)\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names_out()[i], round(topic[i],2))\n",
    "                        for i in topic.argsort()[:-top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc0aea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Topic 0:\n",
      "[('public', 35.28), ('shall', 29.99), ('concern', 21.92), ('state', 21.77), ('member', 19.55), ('decision', 19.31), ('security', 16.35), ('expulsion', 16.12)]\n",
      "--------------------\n",
      "Topic 1:\n",
      "[('member', 32.61), ('state', 31.95), ('eec', 29.33), ('shall', 26.12), ('article', 22.72), ('directive', 21.3), ('union', 20.59), ('right', 19.92)]\n",
      "--------------------\n",
      "Topic 2:\n",
      "[('member', 109.84), ('residence', 105.42), ('state', 105.27), ('shall', 60.89), ('right', 57.71), ('union', 55.04), ('article', 51.85), ('family', 51.28)]\n"
     ]
    }
   ],
   "source": [
    "#Printing topics\n",
    "\n",
    "print_topics(lda_model, vectorizer, top_words = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f0f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
